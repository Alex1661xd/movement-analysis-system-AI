# -*- coding: utf-8 -*-
"""Entrega2_ProyectoFinal_APO3_MovementAnalysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eU32JKruHuilt2meGw315d6oAc52Nz6z

# **Entrega 2 - Proyecto Final APO 3**

## **Sistema de Anotación de Video para Análisis de Actividades Humanas**

$$$$
**Integrantes del grupo:**

*   Mariana De La Cruz - A00399618
*   Valentina Gómez - A00398790
*   Alexis Delgado - A00399176
*   Juan Camilo Amorocho - A00399789

## **Modelado y Entrenamiento del Sistema de Análisis de Movimiento**

### **Contexto**

El proyecto busca aplicar metodologías de analítica de datos y aprendizaje automático para resolver un problema real mediante el procesamiento y análisis de información visual.

El análisis del movimiento humano constituye un desafío técnico que implica la correcta detección y seguimiento de las articulaciones del cuerpo a partir de secuencias de video. Para abordar este reto, se utilizarán herramientas de visión por computadora, que permiten identificar puntos clave del cuerpo y extraer información relevante para su posterior procesamiento.


Este proyecto busca aplicar de manera práctica la metodología CRISP-DM, siguiendo las fases de comprensión, preparación, modelado, evaluación e implementación, con el fin de desarrollar un sistema capaz de reconocer actividades humanas básicas a partir de datos capturados por cámara. De esta forma, se pretende lograr una solución técnica completa que integre todos los pasos del proceso de análisis y clasificación de movimiento, desde la captura de datos hasta la generación de resultados interpretables.

### **Definición del problema**

A pesar de los avances existentes en el reconocimiento de actividades humanas, los sistemas actuales enfrentan dificultades para realizar una clasificación precisa y estable de movimientos en tiempo real, especialmente en condiciones variables de luz, distancia, velocidad y entorno. Estas limitaciones reducen la aplicabilidad de los modelos en escenarios cotidianos o educativos.

En este proyecto se plantea el desarrollo de una herramienta de software capaz de analizar actividades humanas básicas como caminar hacia la cámara, caminar de regreso, girar, sentarse y ponerse de pie mediante el seguimiento de articulaciones clave del cuerpo.

El sistema utilizará la cámara para capturar el movimiento, procesar la información a través de herramientas como MediaPipe y landmarks, extraer características relevantes (velocidades, ángulos e inclinaciones) y finalmente clasificar la actividad ejecutada utilizando modelos de aprendizaje supervisado.

El reto principal consiste en combinar de forma eficiente la detección de pose con la clasificación automática, garantizando que el sistema funcione en tiempo real y sea capaz de adaptarse a diferentes usuarios y condiciones de grabación.

### **Estrategias implementadas para la obtención de nuevos datos**

Con el fin de mejorar la calidad, diversidad y representatividad del conjunto de datos inicial, se implementaron varias estrategias orientadas a capturar nuevos videos y ampliar la cobertura de movimientos y posturas analizadas. En esta segunda fase del proyecto, se buscó reducir el sesgo presente en la primera entrega donde las actividades se limitaban principalmente a caminar hacia adelante, hacia atrás y sentarse incorporando nuevas acciones y variaciones angulares que permitan un análisis más robusto del comportamiento corporal.

En particular, se añadieron videos que muestran movimientos con mayor rango articular, como la flexión y extensión de cadera, el levantamiento de brazos, la inclinación del tronco y variaciones en la dirección del paso. Estos nuevos registros proporcionan información más completa sobre el movimiento tridimensional del cuerpo y enriquecen las métricas derivadas de los landmarks obtenidos con MediaPipe Pose (coordenadas x, y, z de articulaciones clave).

Durante el proceso de recolección, se mantuvieron **condiciones de grabación consistentes** con respecto a iluminación, distancia de cámara y resolución (1080p).

Además, se estandarizó la estructura de carpetas para esta nueva fase del proyecto dentro de `APO3_EntregaFinal/Entrega2/`, que contiene las siguientes secciones:

* `/videos/`: conjunto ampliado de videos con nuevas posturas.
* `/procesados/`: videos con esqueleto 3D superpuesto (MediaPipe Pose).
* `/landmarks/`: archivos CSV con las coordenadas de cada articulación y frame.
* `/resultados/`: métricas numéricas y visualizaciones de los análisis realizados.
* `/modelos/`: modelos entrenados con los nuevos datos y sus evaluaciones.

Esta estrategia permitió generar una base de datos más equilibrada y variada, que constituye un insumo fundamental para la fase de modelado supervisado, ya que posibilita entrenar modelos con mayor poder predictivo y con un entendimiento más completo de la dinámica del movimiento humano.

### **Métricas**

Previo al procesamiento y análisis exploratorio de los videos, se estableció un conjunto de **métricas cuantitativas** diseñadas para evaluar tanto las **características visuales** como los **aspectos biomecánicos** de las grabaciones.
Estas métricas se definieron considerando dos dimensiones principales:

1. La calidad técnica del material (duración, estabilidad, iluminación).
2. Los indicadores corporales derivados del movimiento humano (postura, equilibrio, desplazamiento y ritmo).

Para lograr un análisis más detallado del comportamiento corporal, se integró el modelo **MediaPipe Pose**, capaz de detectar 33 puntos anatómicos (landmarks) del cuerpo humano en tres dimensiones (x, y, z).

A partir de estos landmarks, se calcularon métricas biomecánicas que permiten cuantificar la postura, la alineación y el movimiento de manera más precisa.

De esta forma, el sistema combina información visual y estructural del cuerpo humano, proporcionando una caracterización integral de las actividades registradas.

Las principales métricas utilizadas en este análisis fueron:

* **Velocidad promedio de la cadera:** se calcula con base en el desplazamiento del punto medio entre ambas caderas en cada frame. Permite estimar la estabilidad, el ritmo del movimiento y la fluidez del desplazamiento corporal.
* **Inclinación promedio de los hombros:** diferencia vertical entre los hombros izquierdo y derecho; un valor elevado puede indicar desbalance o desviaciones posturales.
* **Ángulo promedio de la rodilla:** formado por las articulaciones de la cadera, rodilla y tobillo; refleja el grado de flexión o extensión de la pierna, siendo clave para identificar posturas como sentarse o caminar.
* **Movimiento promedio:** mide la variación global entre fotogramas consecutivos, complementando la información de los landmarks para representar el nivel de actividad o cambio corporal.
* **Brillo promedio:** representa la media de la intensidad luminosa de los píxeles; se emplea como control de calidad para garantizar uniformidad en la iluminación de los videos.
* **Duración total del video:** tiempo total en segundos; útil para validar la consistencia temporal entre grabaciones.
* **Número total de frames:** cantidad de fotogramas procesados, directamente relacionada con la duración y la frecuencia de captura.
* **Frecuencia de cuadros por segundo (FPS):** estabilidad temporal del video; en este caso, todos los registros mantienen un promedio constante de **30 FPS**, garantizando coherencia en el muestreo temporal.
* **Velocidad promedio estimada:** indicador derivado del cambio general en la posición de los landmarks, que refleja la intensidad del movimiento global.
* **Inclinación promedio general:** diferencia vertical promedio entre los hombros, empleada como métrica de balance postural a lo largo de toda la secuencia.

En conjunto, estas métricas permiten cuantificar objetivamente la ejecución de las actividades humanas, aportando una base sólida para la clasificación y comparación entre diferentes tipos de movimiento.

El uso de **MediaPipe Pose** no solo incrementa la precisión del análisis, sino que también facilita la interpretación biomecánica de los datos, abriendo la posibilidad de aplicar el modelo en contextos de rehabilitación, ergonomía o análisis deportivo.

Una vez calculadas estas métricas y completada la etapa de entrenamiento, se evaluó el desempeño del modelo SVM utilizando indicadores de rendimiento estadístico. Las métricas consideradas fueron precisión, recall, F1-score y soporte, que permiten medir de forma integral la calidad de las predicciones.

- La precisión indica la proporción de clasificaciones correctas sobre el total de predicciones positivas realizadas.
- El recall mide la capacidad del modelo para reconocer todos los ejemplos reales de cada categoría.
- El F1-score combina la precisión y el recall, equilibrando los errores de omisión y comisión.
- El soporte representa la cantidad de muestras disponibles por cada clase, mostrando que el modelo logró un desempeño ideal.

También se calcularon métricas globales como:
- **Accuracy:** mide el porcentaje total de predicciones correctas sobre todas las muestras del conjunto de prueba.
- **Macro average:** calcula el promedio simple de precisión, recall y F1-score entre todas las clases, sin tener en cuenta el tamaño de cada categoría.
- **Weighted average:** es el promedio ponderado de las mismas métricas, pero considerando la cantidad de ejemplos en cada clase. Suele reflejar mejor el rendimiento global cuando las categorías están desbalanceadas.

En general, los resultados de desempeño confirman que el modelo SVM clasificó de manera exacta todos los movimientos humanos, validando la efectividad del procesamiento y la calidad de las métricas extraídas para este análisis.

## **Datos recolectados**

Los datos utilizados en este proyecto provienen de videos donde cada uno registra a una persona realizando una de las cinco actividades de interés: caminar, girar, sentarse, ponerse de pie y permanecer de pie.
"""

# Limpieza de versiones incompatibles y reinstalación estable
!pip uninstall -y numpy opencv-python opencv-contrib-python opencv-python-headless mediapipe
!pip install numpy==1.26.4 mediapipe==0.10.20 pandas==2.2.2 matplotlib seaborn tqdm opencv-python-headless==4.9.0.80

import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from google.colab import drive
import mediapipe as mp
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, accuracy_score, f1_score
from sklearn.model_selection import train_test_split, GridSearchCV

print("Entorno configurado correctamente y librerías cargadas.")

drive.mount('/content/drive')

BASE_PATH = "/content/drive/MyDrive/APO3_EntregaFinal/Entrega2"
VIDEOS_PATH = os.path.join(BASE_PATH, "videos")
PROCESADOS_PATH = os.path.join(BASE_PATH, "procesados")
LANDMARKS_PATH = os.path.join(BASE_PATH, "landmarks")
RESULTADOS_PATH = os.path.join(BASE_PATH, "resultados")


categorias = ["Adelante", "Atras", "Sentado", "Cadera_Alfrente",
              "Caderas", "Lado", "Sentadilla", "Tijeras"]
total_videos = 0

for categoria in categorias:
    carpeta = os.path.join(VIDEOS_PATH, categoria)
    if not os.path.exists(carpeta):
        print(f"No se encontró la carpeta: {carpeta}")
        continue

    videos = [f for f in os.listdir(carpeta) if f.endswith(('.mp4', '.avi', '.mov'))]
    print(f"{categoria}: {len(videos)} videos encontrados")
    total_videos += len(videos)

print(f"\nTotal de videos encontrados en todas las categorías: {total_videos}")


# Crear carpetas si no existen
for p in [PROCESADOS_PATH, LANDMARKS_PATH, RESULTADOS_PATH]:
    os.makedirs(p, exist_ok=True)

# Subcarpetas por categoría
for categoria in ["Adelante", "Atras", "Sentado", "Cadera_Alfrente",
              "Caderas", "Lado", "Sentadilla", "Tijeras"]:
    os.makedirs(os.path.join(PROCESADOS_PATH, categoria), exist_ok=True)

print("Estructura de carpetas creada correctamente.")

# Configurar MediaPipe Pose

mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)
drawing = mp.solutions.drawing_utils

"""### **Procesamiento visual y extracción de landmarks**

Mediante la librería MediaPipe Pose, se aplicó un pipeline de análisis corporal sobre cada video.

Este proceso realiza, para cada frame, la detección automática de los puntos articulares principales (landmarks) y su conexión en forma de esqueleto 3D.

A su vez, genera versiones procesadas de los videos donde se dibuja la estructura corporal para facilitar la validación visual del modelo.

El código implementado (función `procesar_video_pose`) toma cada archivo de video en `/videos/`, aplica la inferencia del modelo de pose y exporta la versión procesada a la subcarpeta correspondiente de `/procesados/.`

Se mantiene la resolución original de 1080p, con una tasa de muestreo de 30 FPS, y se conserva la coherencia entre las categorías de movimiento.
"""

def calcular_angulo(a, b, c):
    """Calcula el ángulo entre tres puntos (a, b, c)."""
    a, b, c = np.array(a), np.array(b), np.array(c)
    ba, bc = a - b, c - b
    coseno = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)
    angulo = np.degrees(np.arccos(np.clip(coseno, -1.0, 1.0)))
    return angulo

def procesar_video_pose(video_path, output_path):
    """
    Procesa un video aplicando el modelo MediaPipe Pose y genera
    un nuevo video con los landmarks corporales dibujados.
    """
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print(f"[ERROR] No se pudo abrir el video: {video_path}")
        return

    # Configurar parámetros de salida
    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    # Crear objeto Pose con mayor complejidad para precisión 3D
    with mp_pose.Pose(
        static_image_mode=False,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5,
        model_complexity=2
    ) as pose:

        frame_count = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # Convertir a RGB (MediaPipe requiere este formato)
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(image_rgb)

            # Dibujar los landmarks solo si se detecta pose
            if results.pose_landmarks:
                drawing.draw_landmarks(
                    frame,
                    results.pose_landmarks,
                    mp_pose.POSE_CONNECTIONS,
                    drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),
                    drawing.DrawingSpec(color=(255, 255, 255), thickness=1)
                )

            # Escribir el frame procesado
            out.write(frame)
            frame_count += 1

        cap.release()
        out.release()

# --- Procesar todas las subcarpetas ---
categorias = ["Adelante", "Atras", "Sentado", "Cadera_Alfrente",
              "Caderas", "Lado", "Sentadilla", "Tijeras"]

for categoria in categorias:
    carpeta = os.path.join(VIDEOS_PATH, categoria)
    procesados_sub = os.path.join(PROCESADOS_PATH, categoria)
    videos = [f for f in os.listdir(carpeta) if f.endswith(('.mp4', '.avi', '.mov', '.MOV'))]

    print(f"\n Procesando categoría '{categoria}' ({len(videos)} videos)...")
    for v in tqdm(videos):
        input_path = os.path.join(carpeta, v)
        output_path = os.path.join(procesados_sub, v)  # guarda con el mismo nombre
        procesar_video_pose(input_path, output_path)

print("\n Todos los videos fueron procesados y guardados con pose overlay.")

def procesar_video(video_path, categoria, save_landmarks=True):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return None

    fps = cap.get(cv2.CAP_PROP_FPS)
    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duracion = frames / fps if fps > 0 else 0

    # Variables acumulativas
    total_brillo, total_mov = 0, 0
    total_vel, total_incl, total_ang_rod, total_ang_cad, total_ang_tob = 0, 0, 0, 0, 0
    total_dist_hombros_caderas, total_aceleracion, total_simetria = 0, 0, 0
    frame_idx = 0

    landmarks_data = []
    prev_gray, prev_hip, prev_hip_speed = None, None, 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        total_brillo += np.mean(gray)

        # Diferencia entre frames (movimiento general)
        if prev_gray is not None:
            total_mov += np.mean(cv2.absdiff(prev_gray, gray))
        prev_gray = gray

        # Procesar landmarks
        results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        if results.pose_landmarks:
            lm = results.pose_landmarks.landmark

            # Coordenadas principales
            l_shoulder, r_shoulder = np.array([lm[11].x, lm[11].y]), np.array([lm[12].x, lm[12].y])
            l_hip, r_hip = np.array([lm[23].x, lm[23].y]), np.array([lm[24].x, lm[24].y])
            l_knee, l_ankle = np.array([lm[25].x, lm[25].y]), np.array([lm[27].x, lm[27].y])
            l_foot, r_foot = np.array([lm[31].x, lm[31].y]), np.array([lm[32].x, lm[32].y])

            # --- Métricas biomecánicas ---

            # Inclinación hombros
            total_incl += abs(l_shoulder[1] - r_shoulder[1])

            # Ángulos articulares
            total_ang_rod += calcular_angulo(l_hip, l_knee, l_ankle)
            total_ang_cad += calcular_angulo(l_shoulder, l_hip, l_knee)
            total_ang_tob += calcular_angulo(l_knee, l_ankle, l_foot)

            # Distancia entre hombros y caderas
            hombros_mid = (l_shoulder + r_shoulder) / 2
            caderas_mid = (l_hip + r_hip) / 2
            total_dist_hombros_caderas += np.linalg.norm(hombros_mid - caderas_mid)

            # Velocidad y aceleración de cadera
            hip_center = np.mean([l_hip, r_hip], axis=0)
            if prev_hip is not None:
                velocidad_actual = np.linalg.norm(hip_center - prev_hip)
                total_vel += velocidad_actual
                total_aceleracion += abs(velocidad_actual - prev_hip_speed)
                prev_hip_speed = velocidad_actual
            prev_hip = hip_center

            # Simetría corporal (diferencia promedio entre lados izquierdo y derecho)
            diff_izq_der = np.linalg.norm(l_hip - r_hip) + np.linalg.norm(l_shoulder - r_shoulder)
            total_simetria += diff_izq_der

            # Guardar landmarks por frame
            if save_landmarks:
                row = [frame_idx]
                for point in lm:
                    row += [point.x, point.y, point.z, point.visibility]
                landmarks_data.append(row)

        frame_idx += 1

    cap.release()

    # Guardar landmarks CSV por video
    if save_landmarks and landmarks_data:
        cols = ["frame_idx"] + [f"lm{i}_{c}" for i in range(33) for c in ["x", "y", "z", "v"]]
        df_landmarks = pd.DataFrame(landmarks_data, columns=cols)
        output_csv = os.path.join(LANDMARKS_PATH, os.path.basename(video_path).replace('.mp4', '.csv'))
        df_landmarks.to_csv(output_csv, index=False)

    # Resultados promedio del video
    return {
        "categoria": categoria,
        "video": os.path.basename(video_path),
        "fps": round(fps, 2),
        "frames": frames,
        "duracion_seg": round(duracion, 2),
        "resolucion": f"{width}x{height}",
        "brillo_promedio": round(total_brillo / max(frame_idx, 1), 2),
        "movimiento_promedio": round(total_mov / max(frame_idx, 1), 2),
        "velocidad_promedio": round(total_vel / max(frame_idx, 1), 4),
        "aceleracion_promedio": round(total_aceleracion / max(frame_idx, 1), 4),
        "angulo_rodilla_promedio": round(total_ang_rod / max(frame_idx, 1), 2),
        "angulo_cadera_promedio": round(total_ang_cad / max(frame_idx, 1), 2),
        "angulo_tobillo_promedio": round(total_ang_tob / max(frame_idx, 1), 2),
        "inclinacion_promedio": round(total_incl / max(frame_idx, 1), 4),
        "dist_hombros_caderas": round(total_dist_hombros_caderas / max(frame_idx, 1), 4),
        "simetria_promedio": round(total_simetria / max(frame_idx, 1), 4)
    }

resultados = []
categorias = ["Adelante", "Atras", "Sentado", "Cadera_Alfrente",
              "Caderas", "Lado", "Sentadilla", "Tijeras"]

for categoria in categorias:
    carpeta = os.path.join(PROCESADOS_PATH, categoria)
    videos = [f for f in os.listdir(carpeta) if f.endswith(('.mp4', '.avi', '.mov', '.MOV'))]
    print(f"\n Analizando categoría: {categoria} ({len(videos)} videos)")

    for v in tqdm(videos):
        video_path = os.path.join(carpeta, v)
        data = procesar_video(video_path, categoria)
        if data:
            resultados.append(data)

df = pd.DataFrame(resultados)
csv_path = os.path.join(RESULTADOS_PATH, "analisis_videos_con_landmarks.csv")
df.to_csv(csv_path, index=False)

print(f"\n Análisis completado. Resultados guardados en: {csv_path}")
display(df.head())

sns.set(style="whitegrid", palette="muted")

# --- Duración y FPS ---
fig, axes = plt.subplots(1, 2, figsize=(16, 5))
sns.barplot(x="categoria", y="duracion_seg", data=df, ax=axes[0], color="lightblue")
sns.barplot(x="categoria", y="fps", data=df, ax=axes[1], color="lightgreen")
axes[0].set_title("Duración promedio por categoría (segundos)")
axes[1].set_title("FPS promedio por categoría")
for ax in axes:
    ax.tick_params(axis='x', rotation=30)
plt.tight_layout()
plt.show()

# --- Brillo y Movimiento ---
fig, axes = plt.subplots(1, 2, figsize=(16, 5))
sns.barplot(x="categoria", y="brillo_promedio", data=df, ax=axes[0], color="gold")
sns.barplot(x="categoria", y="movimiento_promedio", data=df, ax=axes[1], color="skyblue")
axes[0].set_title("Brillo promedio por categoría")
axes[1].set_title("Movimiento promedio entre frames")
for ax in axes:
    ax.tick_params(axis='x', rotation=30)
plt.tight_layout()
plt.show()

# --- Velocidad y Aceleración ---
fig, axes = plt.subplots(1, 2, figsize=(16, 5))
sns.barplot(x="categoria", y="velocidad_promedio", data=df, ax=axes[0], color="lightgreen")
sns.barplot(x="categoria", y="aceleracion_promedio", data=df, ax=axes[1], color="aquamarine")
axes[0].set_title("Velocidad promedio de cadera")
axes[1].set_title("Aceleración promedio")
for ax in axes:
    ax.tick_params(axis='x', rotation=30)
plt.tight_layout()
plt.show()

# --- Ángulos biomecánicos ---
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
sns.barplot(x="categoria", y="angulo_rodilla_promedio", data=df, ax=axes[0], color="coral")
sns.barplot(x="categoria", y="angulo_cadera_promedio", data=df, ax=axes[1], color="salmon")
sns.barplot(x="categoria", y="angulo_tobillo_promedio", data=df, ax=axes[2], color="tomato")
axes[0].set_title("Ángulo promedio de rodilla (°)")
axes[1].set_title("Ángulo promedio de cadera (°)")
axes[2].set_title("Ángulo promedio de tobillo (°)")
for ax in axes:
    ax.tick_params(axis='x', rotation=30)
plt.tight_layout()
plt.show()

# --- Inclinación, distancia y simetría corporal ---
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
sns.barplot(x="categoria", y="inclinacion_promedio", data=df, ax=axes[0], color="orange")
sns.barplot(x="categoria", y="dist_hombros_caderas", data=df, ax=axes[1], color="khaki")
sns.barplot(x="categoria", y="simetria_promedio", data=df, ax=axes[2], color="violet")
axes[0].set_title("Inclinación promedio de hombros")
axes[1].set_title("Distancia hombros-caderas promedio")
axes[2].set_title("Simetría corporal promedio")
for ax in axes:
    ax.tick_params(axis='x', rotation=30)
plt.tight_layout()
plt.show()

"""Los gráficos reflejan que las métricas calculadas a partir de los landmarks permiten distinguir con claridad las diferentes categorías de movimiento. En general, se observa que:
- En la duración promedio, las categorías con movimientos más controlados o estáticos (como “sentado” y “cadera al frente”) presentaron duraciones intermedias de alrededor de 4 segundos, mientras que “tijeras” registró la mayor duración, reflejando la complejidad y repetitividad del movimiento.

- En cuanto a el FPS promedio fue cercano a 30 en las primeras categorías (“adelante”, “atrás” y “sentado”) y cercano a 60 en las restantes, lo que garantiza una captura fluida y precisa para el cálculo de movimiento.

- El brillo promedio se mantiene relativamente constante, lo que indica que las condiciones de iluminación fueron homogéneas y no afectaron el reconocimiento de landmarks.

- El movimiento promedio entre frames y la velocidad de cadera alcanzan sus valores más altos en la categoría “lado”, seguida por “sentadilla”, mientras que “sentado y tijeras” muestra los valores mínimos, ya que hay falta de desplazamiento en esta postura.

- La aceleración promedio refleja un comportamiento similar, indicando que los ejercicios con mayor componente direccional o cambio de eje presentan una variación temporal más marcada en el movimiento corporal.

- Los ángulos de rodilla, cadera y tobillo son mayores en movimientos como “adelante” y “atrás”, y disminuyen significativamente en “sentadilla” y “lado”, donde las articulaciones se flexionan para ejecutar el movimiento. Esto demuestra que el sistema logra capturar correctamente los cambios angulares derivados de la postura y el tipo de ejercicio.

- La inclinación es más pronunciada en “sentadilla”, evidenciando cambios en el eje corporal durante la ejecución.

- La distancia hombros-caderas se incrementa en movimientos de amplitud amplia como “sentadilla”, mientras que “sentado” mantiene una menor separación.

- En cuanto a la simetría corporal, las categorías “adelante” y “atrás” presentan los valores más altos, indicando equilibrio bilateral, mientras que “cadera al frente” y “lado” muestran mayor asimetría, coherente con posturas laterales o desplazamientos no centrados.

En conclusión, los gráficos muestran una coherencia entre los valores de cada métrica y la naturaleza física del movimiento, validando la capacidad del sistema para diferenciar posturas y patrones de acción mediante los landmarks corporales.
"""

# --- Estadísticas generales ---
display(df.describe())

print("\n Observaciones generales:")
print(f"- Promedio de FPS: {df['fps'].mean():.2f}")
print(f"- Duración promedio: {df['duracion_seg'].mean():.2f} s")
print(f"- Brillo promedio: {df['brillo_promedio'].mean():.2f}")
print(f"- Movimiento promedio: {df['movimiento_promedio'].mean():.2f}")
print(f"- Velocidad promedio de cadera: {df['velocidad_promedio'].mean():.4f}")
print(f"- Aceleración promedio: {df['aceleracion_promedio'].mean():.4f}")
print(f"- Ángulo promedio de rodilla: {df['angulo_rodilla_promedio'].mean():.2f}°")
print(f"- Ángulo promedio de cadera: {df['angulo_cadera_promedio'].mean():.2f}°")
print(f"- Ángulo promedio de tobillo: {df['angulo_tobillo_promedio'].mean():.2f}°")
print(f"- Inclinación promedio de hombros: {df['inclinacion_promedio'].mean():.4f}")
print(f"- Distancia hombros-caderas promedio: {df['dist_hombros_caderas'].mean():.4f}")
print(f"- Simetría corporal promedio: {df['simetria_promedio'].mean():.4f}")

"""### **Análisis de Resultados con Landmarks**

El sistema desarrollado procesa cada cuadro del video utilizando MediaPipe Pose, que detecta un conjunto de landmarks corporales (hombros, codos, caderas, rodillas y tobillos).
A partir de esos puntos de referencia, se ejecuta un conjunto de operaciones matemáticas que permiten medir dinámicamente la postura y el movimiento humano:

- **Captura y procesamiento de video:** Cada grabación tiene un promedio de 41.14 FPS y una duración media de 3.45 segundos, suficientes para representar correctamente la secuencia del movimiento. Los FPS estables aseguran una medición continua del desplazamiento de los landmarks sin pérdida de información temporal.

- **Cálculo de movimiento y velocidad:** A partir de los cambios en la posición de los landmarks de la cadera entre cuadros consecutivos, se calcula el movimiento promedio (2.11), la velocidad (0.0025) y la aceleración (0.0013).  Estos valores reflejan la magnitud y la fluidez del desplazamiento corporal. Las actividades más dinámicas “lado” y “sentadilla” presentan incrementos notables en estas métricas, mientras que las posturas estáticas “sentado” muestran valores bajos.

- **Cálculo de ángulos articulares:** Mediante la posición de tres puntos clave (cadera-rodilla-tobillo), se calculan los ángulos de flexión y extensión usando producto punto entre vectores. Los valores promedio obtenidos 160.4° rodilla, 152.6° cadera y 135.9° tobillo indican que la mayoría de posturas mantienen extensiones parciales, variando según la categoría.

- **Evaluación de alineación corporal y simetría:** La inclinación de hombros (0.0051) y la distancia hombros-caderas (0.149) permiten evaluar la postura general. La simetría corporal promedio (0.217), que es la comparación entre los lados derecho e izquierdo, evidencia un control adecuado del equilibrio y la lateralidad del cuerpo.

En conclusión, el análisis de los landmarks permite reconstruir y comprender la dinámica corporal de manera precisa.

## **Análisis Exploratorio de los Datos**

### **Cargar los datos**
"""

# Cargar datos
csv_path = "/content/drive/MyDrive/APO3_EntregaFinal/Entrega2/resultados/analisis_videos_con_landmarks.csv"
df = pd.read_csv(csv_path)

print("Dimensiones del dataset:", df.shape)
df.head()

df.info()
print("\nValores nulos por columna:\n" , df.isnull().sum())

df.describe().T

"""### **Limpiza de datos**
Elimina columnas redundantes o que no aportan al modelo.
"""

# Eliminar solo las columnas no útiles para el modelado
df_model = df.drop(columns=["video", "resolucion", "fps"])

# Verificar estructura
print("Estructura del dataset limpio para modelado:")
print(df_model.info())
display(df_model.head())

# Guardar dataset limpio
clean_path = os.path.join(RESULTADOS_PATH, "dataset_limpio.csv")
df_model.to_csv(clean_path, index=False)
print(f"Dataset limpio guardado en: {clean_path}")

"""Tras la revisión inicial del dataset generado en la etapa de procesamiento con landmarks, se comprobó que **no existían valores nulos ni inconsistencias** en los registros, lo cual garantizó la calidad del conjunto de datos.

Posteriormente, se realizó una depuración eliminando columnas que no aportaban información relevante al análisis o al futuro modelado, como el nombre del video, la resolución y los FPS, ya que eran constantes o identificadores.

Las métricas seleccionadas para conservar se enfocaron en aquellas que tienen **mayor relación con la postura y el movimiento corporal**, a saber:

* **Brillo promedio:** indicador de las condiciones de iluminación y estabilidad visual del video.
* **Movimiento promedio:** mide la variación de píxeles entre cuadros consecutivos, asociado al grado de actividad.
* **Velocidad promedio:** refleja el desplazamiento del punto medio entre caderas, útil para analizar ritmo y control del movimiento.
* **Ángulo promedio de rodilla:** describe el nivel de flexión o extensión de las piernas, relevante en acciones como sentarse o caminar.
* **Inclinación promedio:** estima el desbalance postural mediante la diferencia vertical entre los hombros.

Estas métricas se consideran las **features principales** que caracterizan las diferentes categorías del movimiento humano. Con esto se conformó un dataset limpio y estructurado, listo para la fase de **análisis exploratorio y modelado supervisado**.

### **Normalización de variables**

Una vez depurado el dataset, se procedió a normalizar las variables numéricas con el objetivo de equilibrar sus escalas y evitar sesgos en el proceso de entrenamiento de los modelos.
Se aplicó el método MinMaxScaler, que transforma los valores de cada variable a un rango entre 0 y 1, manteniendo las proporciones relativas entre observaciones.

Esto fue necesario debido a que las métricas como el brillo promedio (con valores alrededor de 140) y la velocidad promedio (en el orden de 0.002) estaban en magnitudes muy diferentes, lo cual podría afectar el desempeño de algoritmos sensibles a la escala de los datos.

Con esta normalización, las variables quedaron homogéneamente distribuidas, permitiendo que cada una tenga un peso comparable en el análisis y modelado.
De esta manera, el dataset quedó listo para la siguiente fase de entrenamiento de modelos supervisados, asegurando la estabilidad y equidad en el tratamiento de las características numéricas.
"""

# Copia del dataset limpio
df_norm = df_model.copy()

# Seleccionar columnas numéricas (excluyendo la categoría)
cols_numericas = df_norm.select_dtypes(include=["float64", "int64"]).columns
cols_numericas = [c for c in cols_numericas if c != "categoria"]

# Aplicar MinMaxScaler
scaler = MinMaxScaler()
df_norm[cols_numericas] = scaler.fit_transform(df_norm[cols_numericas])

# Mostrar resultados
print("Normalización completada.")
display(df_norm.head())

# Guardar dataset normalizado
norm_path = os.path.join(BASE_PATH, "resultados", "dataset_normalizado.csv")
df_norm.to_csv(norm_path, index=False)
print(f"Dataset normalizado guardado en: {norm_path}")

"""### **Visualización de correlaciones**
Para eliminar redundancia entre métricas, analiza la correlación de variables numéricas.
"""

# Calcular matriz de correlación
corr_matrix = df_norm.select_dtypes(include=["float64", "int64"]).corr()

# Mapa de calor
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("\nMatriz de Correlaciones entre Variables Normalizadas\n")
plt.show()

# Mostrar las correlaciones más altas (excepto con sí mismas)
corr_pairs = (
    corr_matrix.unstack()
    .sort_values(ascending=False)
    .drop_duplicates()
)

print("\n Correlaciones más relevantes (>|0.7|):")
display(corr_pairs[(corr_pairs < 1) & (abs(corr_pairs) > 0.7)])

"""La matriz de correlaciones confirma la consistencia del conjunto de variables cinemáticas, mostrando relaciones esperadas entre métricas de movimiento y ángulos articulares.
Se identifican grupos de variables con comportamientos similares (frames-duración y velocidad-aceleración), lo cual puede ser aprovechado en la etapa de selección de features o reducción de dimensionalidad (por ejemplo, PCA).
La diversidad de correlaciones sugiere que el dataset es informativo y adecuado para entrenar modelos supervisados de clasificación de posturas.

### **Distribución de variables por categoría (Gráficos)**
"""

# Configuración general
sns.set(style="whitegrid", palette="muted")

# Variables biomecánicas clave
variables = [
    ('brillo_promedio', 'Brillo promedio por categoría'),
    ('movimiento_promedio', 'Movimiento promedio por categoría'),
    ('velocidad_promedio', 'Velocidad promedio de cadera por categoría'),
    ('aceleracion_promedio', 'Aceleración promedio de cadera por categoría'),
    ('angulo_rodilla_promedio', 'Ángulo promedio de rodilla por categoría'),
    ('angulo_cadera_promedio', 'Ángulo promedio de cadera por categoría'),
    ('angulo_tobillo_promedio', 'Ángulo promedio de tobillo por categoría'),
    ('inclinacion_promedio', 'Inclinación promedio de hombros por categoría'),
    ('dist_hombros_caderas', 'Distancia hombros–caderas promedio por categoría'),
    ('simetria_promedio', 'Simetría corporal promedio por categoría')
]

# Crear los boxplots
for col, title in variables:
    plt.figure(figsize=(12,6))
    sns.boxplot(x='categoria', y=col, data=df)
    plt.title(f"Distribución de {title}")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

"""1. El primer gráfico muestra la distribución del brillo promedio por categoría, donde se evidencia una variabilidad moderada entre las diferentes posturas analizadas. Las categorías Caderas y Tijeras presentan los valores más altos de brillo promedio, alcanzando aproximadamente entre 155 y 158 unidades, mientras que Adelante, Atrás y Sentado registran los valores más bajos, cercanos a 140–145. Este comportamiento indica que las posturas con mayor extensión corporal o rotación del tronco tienden a reflejar más luz, lo que podría deberse a una mayor exposición de la superficie corporal. En cambio, las posiciones más estáticas o cerradas, como Sentado, muestran un menor brillo.

2. El segundo gráfico, que representa la distribución del movimiento promedio por categoría, se observa una diferencia marcada entre las posturas. La categoría Lado presenta un movimiento promedio considerablemente más alto que las demás, con valores que oscilan entre 5 y 8, lo que indica una actividad corporal más dinámica y amplia. El resto de las categorías se mantiene en niveles bajos, entre 1 y 3, mientras que Sentado presenta los valores más reducidos, próximos a 0.5–0.7. Esto confirma que la posición Lado involucra movimientos de mayor amplitud o desplazamiento lateral, mientras que las categorías Adelante, Atrás y Caderas se asocian a acciones más controladas o con desplazamientos cortos. La categoría Sentado, refleja una postura predominantemente estática, con un estado de mínima movilidad.

3. El tercer gráfico corresponde a la distribución de la velocidad promedio de la cadera por categoría. En este caso, las categorías Lado y Sentadilla presentan los valores más altos, con promedios que alcanzan entre 0.004 y 0.006, lo que evidencia una mayor participación dinámica de la articulación de la cadera. Por el contrario, las categorías Sentado y Tijeras muestran las velocidades más bajas, entre 0.001 y 0.002, indicando un movimiento de baja intensidad en esa región corporal. Las categorías Adelante y Atrás tienen una mayor dispersión en los datos, lo que podría estar relacionado con variaciones individuales en la ejecución o en la intensidad del movimiento.

4. El cuarto gráfico de Distribución de Aceleración promedio de cadera por categoría muestra variaciones claras en la intensidad del movimiento entre las distintas posturas. Las categorías Lado y Sentadilla presentan las aceleraciones más altas, con medianas cercanas a 0.0018, reflejando mayor dinamismo en la cadera.En ambas posturas también se registran algunos valores atípicos que alcanzan hasta 0.0036 y 0.0026, lo cual es por la existencia de ejecuciones rápidas o con variaciones bruscas en el movimiento. La categoría Tijeras registra la aceleración más baja aproximadamente 0.00045, indicando movimientos más controlados. Las posturas Adelante y Atrás mantienen valores intermedios con cierta dispersión, mientras que Cadera al frente, Caderas y Sentado muestran aceleraciones moderadas y estables, con medianas entre 0.0009 y 0.0011. En general, las acciones con mayor desplazamiento corporal generan aceleraciones más elevadas, mientras que las posturas más estáticas conservan un patrón uniforme y de baja variabilidad.

5. El quinto gráfico de Distribución de Ángulo promedio de rodilla por categoría evidencia diferencias notables entre las posturas estáticas y las de mayor flexión. Las categorías Tijeras, Adelante, Atrás y Caderas presentan los ángulos más amplios (superiores a 170°), reflejando piernas casi extendidas y una postura más erguida. En cambio, Sentadilla muestra la mayor flexión de rodilla, con una mediana cercana a 110°, lo que indica un movimiento de alta exigencia articular. Sentado, Cadera al frente y Lado presentan valores intermedios y mayor dispersión, señalando variaciones naturales en la postura. En general, el gráfico confirma que la flexión de rodilla aumenta significativamente en actividades que implican descenso del centro de masa, como sentarse o agacharse, mientras que las posturas verticales mantienen una extensión constante y estable.

6. El sexto gráfico de  Distribución de Ángulo promedio de cadera por categoría muestra una tendencia clara entre extensión y flexión de cadera según la postura. Las categorías Adelante, Atrás, Tijeras y Lado presentan mayores ángulos promedio (alrededor de 160°), lo que indica posturas más extendidas y estables. En cambio, Sentadilla y Cadera al frente reflejan ángulos menores (entre 110° y 125°), asociados a mayor flexión y esfuerzo articular. Sentado tiene una alta variabilidad, esto se debe a las diferencias en la forma de adoptar la postura. En general, el gráfico evidencia que las posiciones con desplazamiento del tronco hacia abajo o al frente requieren una flexión más pronunciada de cadera.

7. El septimo gráfico de Distribución de Ángulo promedio de tobillo por categoría muestra diferencias marcadas en la movilidad del tobillo según la postura. Las categorías Adelante, Sentado, Caderas y Tijeras mantienen ángulos amplios (cercanos a 160°), lo que indica mayor extensión y estabilidad. En cambio, Sentadilla, Lado y Cadera al frente muestran ángulos menores (entre 85° y 110°), asociados a una flexión significativa del tobillo durante el movimiento. La categoría Atrás tiene una alta dispersión, reflejando variaciones en la ejecución. En conclusión, el gráfico muestra cómo las posturas con flexión de rodillas o desplazamiento hacia abajo implican una mayor flexión de tobillo, mientras que las posiciones más erguidas mantienen la articulación extendida.

8. El octavo gráfico de Distribución de Inclinación promedio de hombros por categoría muestra que Sentadilla presenta la mayor inclinación promedio de hombros valores aproximadamente de 0.010 a 0.012, reflejando una fuerte inclinación hacia adelante. Cadera al frente y Atrás mantienen valores intermedios de 0.006 y 0.009, mientras que Adelante y Caderas se sitúan alrededor de 0.004 y 0.007. Por otro lado, Sentado, Lado y Tijeras muestran las inclinaciones más bajas con valores en 0.002 y 0.005, indicando una postura más erguida. Gracias a esto, se observa que las posturas que requieren mayor esfuerzo o flexión profunda generan mayor inclinación del tronco.

9. El noveno gráfico de Distribución de Distancia hombros–caderas promedio por categoría evidencia que Sentadilla y Cadera al frente tienen las mayores distancias promedio entre hombros y caderas con 0.20 y 0.22, reflejando una mayor extensión corporal. En cambio, Sentado presenta la menor distancia con 0.08 y 0.09. Adelante y Atrás se ubican en un rango medio con 0.15 y 0.18, mientras que Caderas, Lado y Tijeras presentan valores intermedios de 0.13 y 0.17. En general, la separación hombros–caderas varía según el grado de estiramiento del tronco en cada postura.

10. El decimo gráfico de Distribución de Simetría corporal promedio por categoría muestra que Adelante y Atrás poseen los niveles más altos de simetría (0.28–0.33), indicando una alineación corporal equilibrada. Tijeras y Caderas mantienen valores medios (0.24–0.27), mientras que Sentado muestra una simetría moderada (0.17–0.20). En cambio, Cadera al frente y Lado presentan los valores más bajos (0.04–0.08), reflejando mayor asimetría. Sentadilla alcanza niveles intermedios (0.08–0.12). En general, las posturas frontales conservan mejor la simetría corporal que las laterales o asimétricas.

## **Modelos**

### **Preparación del dataset para entrenamiento**
"""

# --- Codificar etiquetas ---
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(df_norm["categoria"])

# Mostrar correspondencia
label_map = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
print("Etiquetas codificadas:", label_map)

# --- Separar variables predictoras y objetivo ---
X = df_norm.drop(columns=["categoria"])
y = y_encoded

# --- División del dataset ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

"""### **Entrenamiento de modelos**"""

modelos = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "SVM (RBF)": SVC(kernel="rbf", C=1, gamma="scale", random_state=42),
    "XGBoost": XGBClassifier(
        eval_metric="mlogloss",
        use_label_encoder=False,
        random_state=42
    )
}

"""### **Optimización de Hiperparámetros**"""

# --- Random Forest ---
param_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_rf, cv=3, n_jobs=-1, scoring='accuracy')
grid_rf.fit(X_train, y_train)
best_rf = grid_rf.best_estimator_
print(f"Mejor modelo Random Forest: {grid_rf.best_params_}")

# --- SVM ---
param_svm = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.01, 0.001],
    'kernel': ['rbf']
}

grid_svm = GridSearchCV(SVC(random_state=42), param_svm, cv=3, n_jobs=-1, scoring='accuracy')
grid_svm.fit(X_train, y_train)
best_svm = grid_svm.best_estimator_
print(f"Mejor modelo SVM: {grid_svm.best_params_}")

# --- XGBoost ---
param_xgb = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1]
}

grid_xgb = GridSearchCV(
    XGBClassifier(eval_metric='mlogloss', random_state=42, num_class=len(np.unique(y_train))),
    param_xgb,
    cv=3,
    n_jobs=-1,
    scoring='accuracy'
)
grid_xgb.fit(X_train, y_train)
best_xgb = grid_xgb.best_estimator_
print(f"Mejor modelo XGBoost: {grid_xgb.best_params_}")

"""### **Evaluación de modelos**"""

# --- Entrenar modelos optimizados ---
modelos_opt = {
    "Random Forest (Tuned)": best_rf,
    "SVM (Tuned)": best_svm,
    "XGBoost (Tuned)": best_xgb
}

resultados = []

for nombre, modelo in modelos_opt.items():
    modelo.fit(X_train, y_train)
    y_pred = modelo.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    resultados.append({"Modelo": nombre, "Accuracy": acc})

    print(f"\n- {nombre}")
    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)

    plt.title(f"Matriz de Confusión - {nombre}")
    plt.xlabel("Predicción")
    plt.ylabel("Real")
    plt.tight_layout()
    plt.show()

# --- Comparación de métricas ---
df_resultados = pd.DataFrame(resultados).sort_values(by="Accuracy", ascending=False)
display(df_resultados)

plt.figure(figsize=(7, 4))
sns.barplot(x="Accuracy", y="Modelo", data=df_resultados, hue="Modelo", legend=False, palette="viridis")
plt.title("Comparación de Precisión entre Modelos Tuned")
plt.xlim(0.7, 1.05)
plt.show()

"""- **Interpretación de resultados**

El desempeño sobresaliente de **Random Forest** y **SVM** indica que las características extraídas del análisis de movimiento (como velocidad de cadera, ángulos articulares e inclinación de hombros) son **altamente discriminativas** entre las distintas posturas.
Esto sugiere que el proceso de extracción de características mediante *MediaPipe Pose* y la posterior normalización fueron adecuados para capturar patrones biomecánicos relevantes.

Sin embargo, la precisión perfecta en ambos modelos también puede indicar **posible sobreajuste (overfitting)**, ya que el conjunto de datos aún es relativamente pequeño (86 observaciones totales, 8 clases). En contextos reales o con nuevos sujetos, es probable que el desempeño se reduzca.

El **modelo XGBoost**, aunque potente, requiere mayor cantidad de datos o un ajuste más fino para evitar la pérdida de precisión en clases minoritarias.

## **Plan de Despliegue**

Este plan detalla la implementación de la solución como una aplicación de software funcional. La arquitectura se compondrá de un backend de inferencia (***API REST***) y un frontend de visualización (***Dashboard***), asegurando un sistema robusto y modular.

El despliegue se ejecuta en tres fases:

$$$$

1. **Backend**: API de Inferencia en Tiempo Real
Se desarrollará un microservicio de backend usando FastAPI (o Flask). Este servicio centralizará toda la lógica de procesamiento e inferencia del modelo.

    * **Serialización de Activos:** Se utilizará Joblib para exportar los tres artefactos del modelo entrenados en el notebook: el clasificador SVM/Random Forest (best_svm.joblib), el normalizador (scaler.joblib) y el codificador de etiquetas (label_encoder.joblib).

    * **Lógica de Inferencia**: La API recibirá el flujo de video. Para aplicar el modelo, que fue entrenado con métricas promedio, implementará una lógica de "ventana deslizante" (buffer) que analizará los últimos 1-2 segundos de movimiento.

    * **Procesamiento**: El endpoint de la API ejecutará el siguiente pipeline:

      * Procesará los frames del buffer con MediaPipe Pose.

      * Calculará el vector de features (ángulos, velocidad, inclinación).

      * Normalizará los features y predecirá la actividad con el modelo cargado.

    * **Respuesta**: La API devolverá una respuesta JSON con la clasificación de la actividad (ej. "Sentadilla") y las métricas biomecánicas clave.

2. **Frontend**: Interfaz de Usuario y Dashboard
Se desarrollará una interfaz de usuario (UI) con Streamlit (o un framework web) que actuará como cliente de la API.

    * **Visualización en Vivo**: La UI capturará el video de la cámara web (vía OpenCV) y lo renderizará en la interfaz, superponiendo los landmarks de MediaPipe.

    * **Comunicación con la API**: En segundo plano, el frontend enviará los frames a la API de backend (Paso 1) y recibirá los resultados de la clasificación.

    * **Dashboard de Resultados**: La interfaz mostrará la salida del modelo en tiempo real:

    * **Clasificación de Actividad:** Un panel de texto mostrará la predicción (ej. "Actividad Detectada: Sentadilla").

    * **Métricas Numéricas**: Indicadores visuales mostrarán los ángulos y velocidades, proporcionando retroalimentación cuantitativa.


3. **Persistencia de Datos y Monitoreo**

    Para la mejora continua y la robustez del sistema, se implementará una capa de persistencia de datos.

    * **Base de Datos**: Se integrará una base de datos PostgreSQL conectada al backend.

    * **Registro de Métricas**: La API almacenará las métricas de movimiento clave y las predicciones de cada sesión de análisis.

    * **Retroalimentación del Modelo:** Se implementará una función para registrar nuevos videos y predicciones. Estos datos se usarán para curar un conjunto de datos de re-entrenamiento, mejorando la precisión del modelo y mitigando el sobreajuste detectado.

## **Conclusiones**

En esta segunda entrega se logró ampliar y mejorar el conjunto de datos inicial, incorporando nuevas posturas y movimientos que permitieron un análisis más completo del comportamiento corporal. A partir de los gráficos y métricas obtenidas, se evidenció que los ángulos articulares varían de forma clara según la postura: por ejemplo, Sentadilla y Lado mostraron mayor flexión en rodillas y tobillos, mientras que Adelante y Tijeras presentaron mayor extensión y estabilidad. Esto demuestra que las variables extraídas son capaces de representar con precisión los cambios biomecánicos entre cada tipo de movimiento.

Las variables complementarias, como la inclinación del tronco, la simetría corporal y la distancia entre hombros y caderas, también ayudaron a diferenciar las posturas. En general, las posiciones erguidas fueron más simétricas y estables, mientras que las que implican flexión mostraron más variación y asimetría. Estos resultados confirman que el procesamiento con MediaPipe Pose y las métricas calculadas fueron coherentes y útiles para el análisis.

La normalización de los datos con MinMaxScaler fue un paso importante para equilibrar las distintas escalas de las variables (por ejemplo, brillo y velocidad), evitando que unas tuvieran más peso que otras al momento de entrenar los modelos. Gracias a esto, el dataset quedó más homogéneo y listo para el modelado.

La matriz de correlación mostró relaciones lógicas entre las variables, como entre la velocidad y la aceleración o entre la duración y los frames, lo que confirma la consistencia interna del conjunto de datos. Esto es clave para asegurar que los modelos aprendan de información relevante y no redundante.

Finalmente, los modelos entrenados obtuvieron muy buenos resultados: Random Forest y SVM alcanzaron un 100% de precisión, mientras que XGBoost logró un 88.9%. Aunque esto refleja una excelente capacidad para clasificar las posturas, también puede indicar cierto sobreajuste, probablemente por la cantidad limitada de videos.

En general, esta entrega demuestra que las variables procesadas son muy efectivas para describir y diferenciar posturas, y que el sistema tiene un alto potencial para el reconocimiento automático de movimientos humanos.

### **Impacto del Modelo en el Contexto Real**

El desarrollo de este sistema tiene un impacto potencial importante tanto en el ámbito académico como social:

1. **Contexto universitario:**

   * Fomenta el uso de inteligencia artificial aplicada al análisis biomecánico y visión por computador.
   * Permite a estudiantes e investigadores medir posturas, movimientos y ángulos articulares sin necesidad de equipos costosos de captura de movimiento.
   * Abre la puerta a proyectos interdisciplinarios entre ingeniería, fisioterapia, deporte y ergonomía.

2. **Aplicaciones prácticas y sociales:**

   * **Prevención de lesiones:** el sistema podría alertar sobre movimientos incorrectos o posturas forzadas en tiempo real.
   * **Rehabilitación y fisioterapia:** podría emplearse para hacer seguimiento del progreso de pacientes mediante métricas cuantitativas.
   * **Educación física o entrenamiento deportivo:** permite evaluar la técnica en ejercicios o posturas específicas (sentadilla, estiramientos, levantamientos).
   * **Entornos laborales:** útil para análisis ergonómico y detección de malas posturas repetitivas en trabajadores.

3. **Escalabilidad e impacto futuro:**

   * Con una base de datos más grande y diversa, el sistema podría escalar hacia una aplicación móvil o web, incorporando análisis en tiempo real con cámaras RGB.
   * Se podrían incluir módulos de **detección de errores posturales**, feedback visual y sugerencias automáticas para corregir el movimiento.
"""